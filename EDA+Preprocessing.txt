import polars as pl
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter
import seaborn as sns
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer

# Download necessary NLTK data
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Load dataset
df = pl.read_csv("your_dataset.csv")  # Update with actual path

# Basic Data Overview
print("Dataset Shape:", df.shape)
print("Column Names:", df.columns)
print("Missing Values:")
print(df.null_count())
print("Duplicate Rows:", df.filter(df.is_duplicated()).shape[0])

# Convert categorical columns
df = df.with_columns(
    pl.col("Category").cast(pl.Categorical),
    pl.col("Sub-category").cast(pl.Categorical)
)

# Initialize NLTK utilities
STOPWORDS = set(stopwords.words('english'))
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

# Text Preprocessing Functions
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)  # Remove special characters
    return text

def remove_stopwords(text):
    return " ".join([word for word in text.split() if word not in STOPWORDS])

def stem_text(text):
    return " ".join([stemmer.stem(word) for word in text.split()])

def lemmatize_text(text):
    return " ".join([lemmatizer.lemmatize(word) for word in text.split()])

# Apply Preprocessing
df = df.filter(pl.col("Description").is_not_null()).with_columns(
    pl.col("Description").map_elements(clean_text).alias("cleaned_description")
)
df = df.with_columns(
    pl.col("cleaned_description").map_elements(remove_stopwords).alias("no_stopwords_description")
)
df = df.with_columns(
    pl.col("no_stopwords_description").apply(stem_text).alias("stemmed_description"),
    pl.col("no_stopwords_description").apply(lemmatize_text).alias("lemmatized_description")
)

# Description Length Analysis
df = df.with_columns(
    pl.col("cleaned_description").str.len_chars().alias("description_length"),
    pl.col("cleaned_description").str.len_bytes().alias("description_bytes"),
    pl.col("cleaned_description").str.n_chars().alias("word_count")
)

# Plot distribution of description lengths
plt.figure(figsize=(10, 5))
sns.histplot(df["description_length"].to_list(), bins=50, kde=True)
plt.title("Distribution of Description Lengths")
plt.xlabel("Number of Characters")
plt.ylabel("Frequency")
plt.show()

# Word Frequency Analysis
def get_word_counts(texts):
    words = " ".join(texts).lower().split()
    return Counter(words)

word_counts = get_word_counts(df["cleaned_description"].drop_nulls().to_list())

# Word Cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_counts)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

# Category Distribution
plt.figure(figsize=(12, 6))
sns.countplot(y=df["Category"].to_list(), order=df["Category"].value_counts().keys(), palette="viridis")
plt.title("Category Distribution")
plt.xlabel("Count")
plt.ylabel("Category")
plt.show()

# Sub-Category Distribution
plt.figure(figsize=(12, 6))
sns.countplot(y=df["Sub-category"].to_list(), order=df["Sub-category"].value_counts().keys(), palette="coolwarm")
plt.title("Sub-category Distribution")
plt.xlabel("Count")
plt.ylabel("Sub-category")
plt.show()

# Category vs. Description Length
plt.figure(figsize=(12, 6))
sns.boxplot(x=df["Category"].to_list(), y=df["description_length"].to_list())
plt.xticks(rotation=45)
plt.title("Description Length by Category")
plt.xlabel("Category")
plt.ylabel("Description Length")
plt.show()

# Correlation Between Categories and Sub-categories
category_subcategory_counts = df.groupby(["Category", "Sub-category"]).count().sort(by="count", descending=True)
print(category_subcategory_counts)

# Checking for Most Common Bigrams (Two-word phrases)
def get_bigrams(texts):
    bigrams = Counter()
    for text in texts:
        words = text.lower().split()
        bigrams.update([(words[i], words[i+1]) for i in range(len(words)-1)])
    return bigrams

bigrams = get_bigrams(df["cleaned_description"].drop_nulls().to_list())
print("Most Common Bigrams:", bigrams.most_common(10))

# Word Count Distribution per Category
plt.figure(figsize=(12, 6))
sns.boxplot(x=df["Category"].to_list(), y=df["word_count"].to_list())
plt.xticks(rotation=45)
plt.title("Word Count Distribution per Category")
plt.xlabel("Category")
plt.ylabel("Word Count")
plt.show()

# Missing Value Heatmap
plt.figure(figsize=(10, 5))
sns.heatmap(df.null_count().to_pandas().T, cmap="viridis", cbar=False, annot=True)
plt.title("Missing Value Heatmap")
plt.show()

# Outlier Detection in Description Length
q1 = np.percentile(df["description_length"].to_list(), 25)
q3 = np.percentile(df["description_length"].to_list(), 75)
iqr = q3 - q1
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr
outliers = df.filter((pl.col("description_length") < lower_bound) | (pl.col("description_length") > upper_bound))
print("Outliers in Description Length:", outliers.shape[0])
